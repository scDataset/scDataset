{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebcf6f7",
   "metadata": {},
   "source": [
    "# Experiment Results Analysis\n",
    "\n",
    "This notebook loads experiment results and generates plots comparing different data loading strategies across the four classification tasks:\n",
    "- Cell line classification\n",
    "- Drug classification\n",
    "- MOA (broad) classification\n",
    "- MOA (fine) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "notebook_dir = Path().absolute()\n",
    "project_root = notebook_dir.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import analysis utilities\n",
    "from training_experiments.analysis import (\n",
    "    load_experiment_results,\n",
    "    extract_metrics,\n",
    "    create_comparison_dataframe,\n",
    "    get_time_comparison,\n",
    "    print_results_table,\n",
    "    STRATEGY_DISPLAY_NAMES,\n",
    "    TASK_DISPLAY_NAMES,\n",
    "    DEFAULT_YLIM,\n",
    ")\n",
    "from training_experiments.analysis.plotting import (\n",
    "    plot_comparison,\n",
    "    plot_training_curves,\n",
    "    plot_time_comparison,\n",
    ")\n",
    "\n",
    "# Set default plot style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"âœ“ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f853a6c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to your experiment results directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to experiment results\n",
    "# Update this to point to your results directory\n",
    "RESULTS_DIR = notebook_dir.parent / \"results\"\n",
    "\n",
    "# Alternative: absolute path\n",
    "# RESULTS_DIR = Path(\"/home/kidara/raid/volume/scDataset/training_experiments/results\")\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Exists: {RESULTS_DIR.exists()}\")\n",
    "\n",
    "if RESULTS_DIR.exists():\n",
    "    print(\"\\nContents:\")\n",
    "    for p in sorted(RESULTS_DIR.iterdir()):\n",
    "        print(f\"  {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4913b",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load all experiment results from the results directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bc11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "results = load_experiment_results(RESULTS_DIR)\n",
    "\n",
    "print(f\"Loaded results for {len(results)} strategies:\")\n",
    "for strategy in results:\n",
    "    status = results[strategy].get('status', 'unknown')\n",
    "    print(f\"  - {strategy}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ac342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results summary table\n",
    "if results:\n",
    "    print_results_table(results, metric_name='f1_macro')\n",
    "else:\n",
    "    print(\"No results to display. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c379088",
   "metadata": {},
   "source": [
    "## Manual Results Entry (Optional)\n",
    "\n",
    "If results files are not available, you can manually enter results here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58710ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Manually define results if not loading from files\n",
    "# Uncomment and modify this section if needed\n",
    "\n",
    "# manual_data = [\n",
    "#     # (Task, Method, Macro F1-score, Error)\n",
    "#     ('Cell line', 'Streaming', 0.85, 0.02),\n",
    "#     ('Cell line', 'Streaming (buffer)', 0.86, 0.02),\n",
    "#     ('Cell line', 'Block size = 4\\nFetch factor = 16', 0.88, 0.01),\n",
    "#     # ... add more rows\n",
    "# ]\n",
    "\n",
    "# manual_df = pd.DataFrame(manual_data, columns=['Task', 'Method', 'Macro F1-score', 'Error'])\n",
    "\n",
    "USE_MANUAL_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ead1f",
   "metadata": {},
   "source": [
    "## Create Comparison DataFrame\n",
    "\n",
    "Prepare data for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results and not USE_MANUAL_DATA:\n",
    "    # Create DataFrame from loaded results\n",
    "    df = create_comparison_dataframe(results, metric_name='f1_macro')\n",
    "    print(f\"Created comparison DataFrame with {len(df)} rows\")\n",
    "    display(df)\n",
    "elif USE_MANUAL_DATA:\n",
    "    df = manual_df\n",
    "    print(f\"Using manual data with {len(df)} rows\")\n",
    "else:\n",
    "    print(\"No data available. Please either:\")\n",
    "    print(\"  1. Run experiments to generate results\")\n",
    "    print(\"  2. Enable USE_MANUAL_DATA and fill in manual_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceff2b7",
   "metadata": {},
   "source": [
    "## Main Comparison Plot\n",
    "\n",
    "Generate the 2x2 comparison plot for all classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51799d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom y-axis limits for each task\n",
    "# Adjust these based on your results\n",
    "ylim_dict = {\n",
    "    'Cell line': (0.0, 1.0),\n",
    "    'Drug': (0.0, 0.1),\n",
    "    'MOA (broad)': (0.0, 0.3),\n",
    "    'MOA (fine)': (0.0, 0.15),\n",
    "}\n",
    "\n",
    "# Generate the comparison plot\n",
    "if 'df' in dir() and len(df) > 0:\n",
    "    fig, axes = plot_comparison(\n",
    "        df,\n",
    "        ylim_dict=ylim_dict,\n",
    "        figsize=(14, 10),\n",
    "        title=\"Data Loading Strategy Comparison\"\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = RESULTS_DIR / \"comparison_plot.png\"\n",
    "    fig.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nFigure saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7d1c6",
   "metadata": {},
   "source": [
    "## Training Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training time comparison\n",
    "if results:\n",
    "    time_df = get_time_comparison(results)\n",
    "    \n",
    "    if len(time_df) > 0:\n",
    "        fig, ax = plot_time_comparison(time_df)\n",
    "        \n",
    "        # Save figure\n",
    "        output_path = RESULTS_DIR / \"time_comparison.png\"\n",
    "        fig.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Figure saved to: {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No timing data available\")\n",
    "else:\n",
    "    print(\"No results loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1888573",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad099f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for each task\n",
    "if results:\n",
    "    tasks = ['cell_line', 'drug', 'moa_broad', 'moa_fine']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, task in enumerate(tasks):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for strategy, result in results.items():\n",
    "            if result.get('status') != 'success':\n",
    "                continue\n",
    "            \n",
    "            history = result.get('history', {})\n",
    "            metric_key = f'val_{task}_f1_macro'\n",
    "            \n",
    "            if metric_key in history:\n",
    "                values = history[metric_key]\n",
    "                epochs = list(range(1, len(values) + 1))\n",
    "                label = STRATEGY_DISPLAY_NAMES.get(strategy, strategy).replace('\\n', ' ')\n",
    "                ax.plot(epochs, values, label=label, marker='o', markersize=4)\n",
    "        \n",
    "        task_name = TASK_DISPLAY_NAMES.get(task, task)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Macro F1-score')\n",
    "        ax.set_title(task_name)\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Training Curves by Task', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = RESULTS_DIR / \"training_curves.png\"\n",
    "    fig.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Figure saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17fd9c",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e88035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison data to CSV\n",
    "if 'df' in dir() and len(df) > 0:\n",
    "    csv_path = RESULTS_DIR / \"comparison_results.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results exported to: {csv_path}\")\n",
    "    \n",
    "    # Also create a pivot table\n",
    "    pivot_df = df.pivot(index='Method', columns='Task', values='Macro F1-score')\n",
    "    pivot_path = RESULTS_DIR / \"comparison_pivot.csv\"\n",
    "    pivot_df.to_csv(pivot_path)\n",
    "    print(f\"Pivot table exported to: {pivot_path}\")\n",
    "    \n",
    "    print(\"\\nPivot Table:\")\n",
    "    display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6ef0d",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "if 'df' in dir() and len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best method per task\n",
    "    print(\"\\nBest method per task:\")\n",
    "    for task in df['Task'].unique():\n",
    "        task_data = df[df['Task'] == task]\n",
    "        best_idx = task_data['Macro F1-score'].idxmax()\n",
    "        best_row = df.loc[best_idx]\n",
    "        print(f\"  {task}: {best_row['Method'].replace(chr(10), ' ')} (F1 = {best_row['Macro F1-score']:.4f})\")\n",
    "    \n",
    "    # Overall method comparison\n",
    "    print(\"\\nMean performance across all tasks:\")\n",
    "    mean_by_method = df.groupby('Method')['Macro F1-score'].mean().sort_values(ascending=False)\n",
    "    for method, score in mean_by_method.items():\n",
    "        print(f\"  {method.replace(chr(10), ' ')}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b25a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
